{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import yaml\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Configure SparkSession\n",
    "spark = SparkSession.builder.appName(\"Raw to STG Transformation\").getOrCreate()\n",
    "dbutils = DBUtils(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load database connection details from Databricks secrets\n",
    "secret_scope = \"gustavo_lima_adw\"\n",
    "mssql_host = dbutils.secrets.get(secret_scope, \"mssql_host\")\n",
    "mssql_port = dbutils.secrets.get(secret_scope, \"mssql_port\")\n",
    "mssql_database = dbutils.secrets.get(secret_scope, \"mssql_database\")\n",
    "username = dbutils.secrets.get(secret_scope, \"username\")\n",
    "password = dbutils.secrets.get(secret_scope, \"password\")\n",
    "schema_source = \"Sales\"\n",
    "\n",
    "# Define JDBC connection properties\n",
    "connection_properties = {\n",
    "    \"user\": username,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "    \"encrypt\": 'true',\n",
    "    \"trustServerCertificate\": 'true'\n",
    "}\n",
    "jdbc_url = f\"jdbc:sqlserver://{mssql_host}:{mssql_port};databaseName={mssql_database};encrypt=true;trustServerCertificate=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transformations.yml file\n",
    "yaml_path = \"/Workspace/Users/gustavo.lima@indicium.tech/.bundle/desafio_lh/prod/files/src/transformations.yml\"\n",
    "try:\n",
    "    with open(yaml_path, \"r\") as file:\n",
    "        transformations = yaml.safe_load(file)\n",
    "        tables_metadata = {table[\"name\"]: table[\"columns\"] for table in transformations[\"sources\"][0][\"tables\"]}\n",
    "        print(\"Successfully loaded transformation metadata.\")\n",
    "except FileNotFoundError:\n",
    "    raise Exception(f\"YAML file not found at path: {yaml_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(column_name):\n",
    "    \"\"\"Converte CamelCase para snake_case incluindo tratamento de números e siglas.\"\"\"\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', column_name)\n",
    "    s2 = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1)\n",
    "    s3 = re.sub('([a-z])([0-9])', r'\\1_\\2', s2)  # Trata números (ex: YTD)\n",
    "    s4 = s3.replace(' ', '_').replace('-', '_')\n",
    "    return re.sub('_+', '_', s4).lower()\n",
    "\n",
    "def transform_table(df, table_name):\n",
    "    \"\"\"Aplica snake_case e type casting baseado no YAML.\"\"\"\n",
    "    if table_name not in tables_metadata:\n",
    "        print(f\"Warning: No metadata for {table_name}. Skipping.\")\n",
    "        return df\n",
    "\n",
    "    transformations = tables_metadata[table_name]\n",
    "    \n",
    "    for column in transformations:\n",
    "        original_name = column[\"name\"]\n",
    "        new_name = to_snake_case(original_name)\n",
    "        \n",
    "        # Debug: Verifica conversão\n",
    "        print(f\"Renomeando coluna: {original_name} -> {new_name}\")\n",
    "        \n",
    "        # Renomeia primeiro\n",
    "        df = df.withColumnRenamed(original_name, new_name)\n",
    "        \n",
    "        # Type casting\n",
    "        if \"data_tests\" in column:\n",
    "            description = column.get(\"description\", \"\").lower()\n",
    "            if \"int\" in description:\n",
    "                df = df.withColumn(new_name, col(new_name).cast(\"int\"))\n",
    "            elif \"string\" in description:\n",
    "                df = df.withColumn(new_name, col(new_name).cast(\"string\"))\n",
    "            elif \"date\" in description:\n",
    "                df = df.withColumn(new_name, col(new_name).cast(\"date\"))\n",
    "            elif \"timestamp\" in description:\n",
    "                df = df.withColumn(new_name, col(new_name).cast(\"timestamp\"))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process XML columns in the dataset\n",
    "def process_xml_column(df, xml_column, extracted_columns):\n",
    "    \"\"\"\n",
    "    Processes an XML column in a DataFrame and extracts specified fields.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing the XML column.\n",
    "        xml_column (str): Name of the column containing XML data.\n",
    "        extracted_columns (list): List of tuples with field names and their corresponding XPath.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with extracted XML fields as separate columns.\n",
    "    \"\"\"\n",
    "    for field_name, xpath in extracted_columns:\n",
    "        df = df.withColumn(field_name, expr(f\"xpath_string({xml_column}, '{xpath}')\"))\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name in tables_metadata.keys():\n",
    "    print(f\"\\nProcessando tabela: {table_name}\")\n",
    "    \n",
    "    # Força recriação da tabela\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS gustavo_lima_stg.schema.{table_name}\")\n",
    "    \n",
    "    # Leitura do RAW\n",
    "    df = spark.sql(f\"SELECT * FROM gustavo_lima_raw.schema.{table_name}\")\n",
    "    \n",
    "    # Aplica transformações\n",
    "    transformed_df = transform_table(df, table_name)\n",
    "    \n",
    "    # Processamento de XML (exemplo para tabela 'store')\n",
    "    if table_name == \"store\":\n",
    "        xml_fields = [\n",
    "            (\"annual_sales\", \"/StoreSurvey/AnnualSales/text()\"),\n",
    "            (\"annual_revenue\", \"/StoreSurvey/AnnualRevenue/text()\"),\n",
    "            (\"bank_name\", \"/StoreSurvey/BankName/text()\"),\n",
    "            (\"business_type\", \"/StoreSurvey/BusinessType/text()\"),\n",
    "            (\"year_opened\", \"/StoreSurvey/YearOpened/text()\"),\n",
    "            (\"square_feet\", \"/StoreSurvey/SquareFeet/text()\")\n",
    "        ]\n",
    "        transformed_df = process_xml_column(transformed_df, \"demographics\", xml_fields)\n",
    "    \n",
    "    # Salva no STG com overwrite completo\n",
    "    transformed_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(f\"gustavo_lima_stg.schema.{table_name}\")\n",
    "    \n",
    "    print(f\"Tabela {table_name} salva no STG.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nValidação das Tabelas STG:\")\n",
    "spark.sql(\"SHOW TABLES IN gustavo_lima_stg.schema\").show()\n",
    "spark.sql(\"DESCRIBE gustavo_lima_stg.schema.salesorderdetail\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
